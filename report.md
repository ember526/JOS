##1 线程的结构与基本操作
我对于JOS中线程的设计和Linux系统中对线程的观点类似：线程和进程在本质上是完全相同的（他们都被称作是任务），他们的区别在于对资源的共用情况不同，其中最重要的资源就是地址空间。在X86体系中地址空间的标志就是位于cr3寄存器中的页表地址，这个页表地址指向一级页表，一级页表中保存了二级页表的地址，而二级页表中的条目指向了真正的物理页，所有的页表都受内核的管理，位于地址空间的页表区。也就是说，在创建新的线程的时候，地址空间这一核心资源完全由任务管理块的一级页表地址指明，这样子就完成了主线程和新线程之间地址空间的共用，他们对地址访问的权限是完全一致的。在多核架构下，地址空间的管理实际上还有很多的问题需要解决，这些问题需要在后面继续讨论。

###创建进程
线程作为系统资源，对其操作理所当然是要交付给操作系统来完成。用户程序通过系统调用来完成对资源的申请，在创建进程过程中，需要发起sys_clone系统调用。在JOS中，sys_clone完成对线程控制块的初始化，线程资源的分配，以及线程上下文的初始化。在JOS的设计中，每一个进程可以申请数量为THREADSNM的线程，子线程控制块的指针保存在父进程控制块的定长数组里，其位置决定了线程的线程号tid。在确定了子线程控制块指针的位置之后，系统调用初始化控制块里的一系列成员，包括段寄存器的初始值，请求特权等级RPL，用户栈的位置，允许中断位等。另外还有一些特别的成员需要特殊说明。因为新的线程和父线程是共用地址空间的，所以只需要将父进程的页表地址赋给新线程即可，之后所有线程对地址空间做出的更改都是进程间互相可见的。不过用户栈显然不是可以公用的，为了新线程能够正确的执行代码，在线程创建的时候还需要为之申请映射独有的用户栈；新的用户栈可以直接按照tid来安排在USTACKTOP（也就是主线程用户栈顶）的下方。在JOS中我对每个进程分配了4k（也就是一个页的大小）的栈空间，除此之外每两个线程的用户栈之间还需要一个空白的区域来作为保护区，当一个线程执行的代码出现栈溢出的时候，就会及时的撞到保护区触发无效访问的页错误而不会破坏其他线程的用户栈。按照这个栈空间的设计，就可以得到每一个线程对应用户栈的栈顶，从而可以确定线程上下文中esp的值。

不管是进程创建还是线程创建，创建者在使用系统调用创建好新的进程/线程之后，调度例程在之后将会调度它们，执行流在刚刚到达他们要执行的代码的时候，它们不会因为自己是新创建的而表现得有所不同，甚至不会觉察到在刚刚控制曾经到达了内核中；它们之所以会自然的执行，就是因为在内核代码中，其控制块被精巧的设计好了。对于线程而言，控制交付给它的时候要执行代码的地址，是sys_clone中fcn参数的值；为了做到这一点，系统调用只需要将新线程上下文结构体中的eip值设置为这个值即可。那么这个函数的参数从哪里得到的呢？根据C语言的调用规则，在刚进入函数的那一刻，返回地址在栈的顶部，参数就在返回地址的上面，所以系统调用需要将线程例程的参数放在相应的位置。这个时候线程的创建已经顺利完成了，内核代码将线程标记成RUNNABLE，等待之后的调度。

###终止进程
可是这个时候的线程是没有办法正常终止的。线程创建的时候就是去执行一个函数，函数如何终止似乎不那么显然。我们知道进程终止和程序终止是一致的，通过exit()函数回收相应的进程资源；实际上线程的终止和进程也是类似的。按照C语言的调用规则，函数在执行return指令之后，会恢复esp和ebp的值，同时根据栈中返回地址的值来跳转到调用函数。刚刚我提到了，返回地址实际上就是在进入函数时的栈顶处。因此在创建线程的时候，我可以将合适的函数地址放置到这个位置上；这个函数需要完成线程资源的回收与释放，显然这些工作是需要内核完成的，这也就引发了一些小问题。当用户线程将例程执行完了之后，执行return指令，我希望它返回到我创建进程时精心设计的退出例程代码的位置上；可是这样子就会触发页错误，原因是用户线程试图执行内核代码。我们知道退出例程一定是需要内核来完成的，为了解决这个问题，自然的想到要设计一个sys_thread_destroy的系统调用，然后用库函数来包装这个系统调用，将库函数的地址放在线程例程的返回地址处；由于内核在创建线程的时候并不知道这个销毁线程函数的地址，所以设定返回地址的任务实际上需要由用户程序来完成，我为JOS添加了一个sys_thread_set_rtn_routine系统调用来完成这个工作，也就是说现在用户程序可以根据需要来自己定义线程执行完成之后做哪些其他工作，而不是立即终止线程。这和unix中pthread_cleanup函数很类似。线程的终止例程是创建例程的逆过程，释放线程的控制块，同时将线程栈占用的虚拟页面释放掉，最后将线程占用的进程控制块释放掉。

###phread_join()
在完成了线程的创建和终止之后，线程的工作实际上还是没有办法正常完成的，原因是主线程一旦退出，操作系统就会回收主线程的全部资源。在JOS中，主要的资源包括地址空间以及执行流，子线程的执行流这个时候不会因为主线程的退出而退出，但是地址空间会因为主线程的资源释放而变得无效，这个时候子线程的任何地址访问（包括取出指令）都将会触发页错误。为了避免情况下出现页错误，需要在每一个进程退出的时候主动将所有附属于自己的线程全部终止掉。为了能够让主线程等待子线程继续执行，需要给用户程序提供pthread_join()函数，当子线程终止之前阻塞住程序的继续执行。在设计JOS系统的时候，我们规定在线程退出的时候将主线程中指向该线程的指针置空，那么我们就可以通过检查相应的指针来判断子线程是否退出了，从而决定是否继续执行。这里的一个问题就是，阻塞行为是不可以放在JOS的内核中进行的，原因是如果在内核中放入循环，由于JOS的内核同一时间只能由一个线程请求占用，那么将会导致子线程永远不可能通过结束线程的系统调用终止，那么操作系统将会陷入死循环。我们只能够通过反复进行系统调用来完成阻塞，可以通过在系统调用的最后调用sched_yield函数来及时交出时间片的方式对这一过程进行优化。


###多核架构
线程对于提升用户程序性能的一个原因是，用户程序能够通过线程调度多个处理器的计算功能，所以给操作系统增加对多核处理器的支持是实现线程的重要一环，否则线程所能起到的功效就会大打折扣。多核架构不是硬件提供的一层抽象，操作系统实际上需要利用硬件提供的设施来完成多任务的基础设施。

JOS采用的的是SMP，对称多处理架构，所有的处理器在资源的占有和起到的功能是一样的。不过在引导阶段，CPU还是承担不同的角色————不是所有的处理器都是同步初始化的。有一个处理器为BSP，即引导处理器；其他的处理器被称为应用处理器（AP），都受BSP的激活。哪一个处理器为BSP是受BIOS和硬件决定的。在BSP启动之后，进入内核初始化阶段，此时调用有关的函数来激活BSP。首先相关的函数是mp_init()，这个函数从硬件中初始化多处理器的相关信息。

然后是lapic_init()函数，初始化LAPIC（Local Advanced Programmable Interrupts Controller）。APIC在X86架构下担任有关中断信息传递的功能，不过在这里我们关注的lapic是APIC的一部分，它承担的工作是将IPI（Interprocessor Interrput）信息通过系统总线传递给各个处理器。一般来说，IPI能够用来向多个处理器分布中断信息，或者是执行系统函数（比如这里的启动AP）。回想由系统调用引发的中断，程序在INT指令之后，系统根据中断号计算出中断矢量表中相应中断的入口，并在内核模式下开始执行中断处理函数。而在处理器之间的中断传递是通过Memory-mapped I/O(MMIO)进行的，MMIO将IO设施以与内存同样的方式进行编址，从而可以用访存的方式对设备的寄存器进行访问。处理器的lapic位于物理地址0xFE000000的位置，占用MMIO中4k的空间，在JOS中我们实现了mmio_map_region()函数来为lapic在虚拟空间中留出一个位置，并将其映射到这里。

接下来就要开始进入启动AP的阶段了。在激活其他处理器之前，BSP首先要调用lock_kernel()锁住内核代码，在JOS中内核代码同一时间内只能有一个处理器执行。然后调用boot_aps()启动AP，该函数要做的第一件事情是将启动代码（由mpentry_start符号标记开始位置）移动到MPENTRY_PADDR的位置，之所以要在这个位置启动是为了满足处理器启动时的限制（比如处理器启动的时候需要将DS清零，所以指令地址应该在低2^16字节内）。然后对于mp_init()函数中获取的每一个cpu指定KERNBASE下方的堆栈，堆栈的位置由CPU的序号决定；堆栈在虚拟地址空间的位置已经在mem_init()函数中映射过了。接下来就可以开始执行lapic_startap()执行启动了，这里代码的细节是英特尔的启动算法例程，不再作更进一步的说明了。

##2 自旋锁
自旋锁是操作系统中最为简单轻便的锁设施，因为它不涉及到线程的挂起，本质是通过线程上执行循环操作来等待需要的系统资源释放。在几乎全部的锁设施中都会存在的一个概念就是锁变量，一般以锁变量大于0来表示锁变量对应的资源是空闲可用的，其他状态表示被占用的。但是在涉及到多线程的场景下，竞争状态是出现在所有地方的，包括对锁变量的操作。那么对锁变量进行原子性操作，就需要借助到硬件层面的支持。因此在关于锁的实现和讨论中，有很大一部分是和硬件相关的。

实现自旋锁的核心部分就是x86的xchg指令，需要用到嵌入C的汇编代码。在项目的x86.h头文件中对其进行了一些封装，其参数为一个指针变量，指向的是要变更的锁变量，以及锁变量的新值。对于旋转锁而言，锁变量的值在0和1之间来回跳变。一次成功的上锁只有当锁变量是0的时候才能发生，线程调用xchg(&lock, 1)将锁变量从0置换为1，意味着一处代码占用了资源；解锁操作永远是成功的，因为不管锁变量此时的值是多少，调用xchg(&lock, 0)将其置为0即可。所以主要的问题出现在上锁操作：如何确定上锁操作成功了呢？库函数需要通过某种方法知道锁操作是否成功了，只有这样才能够实现我们对自旋锁的期望：当加锁不成功时反复尝试直到一次加锁成功。内联汇编可以指定指令的返回值，一般来说格式是这个样子的。
asm ( "statements" : output_registers : input_registers : clobbered_registers);
xchg()函数在执行完xchg指令之后，将获取换出来的值作为返回值，返回这个返回值，这样子以来pthread_spinlock_lock()函数通过检查返回值是否为0就能够知道锁操作是否成功了。

梳理一下关于自旋锁相关api的实现。首先pthread_spin_init()函数将指定锁变量的初始值置为0，表示初始状态下资源是空闲的，任何线程都可以对该自旋锁加锁；pthread_spin_lock()函数通过调用xchg(&lock, 1)，在xchg内部通过一个循环反复将锁变量的当前值和一个值为1的变量互换，直到xchg()返回0为止；pthread_spin_unlock()函数通过调用xchg(&lock, 0)，直接完成解锁操作，将锁变量的值置为0。另外在上锁操作的循环中会执行“pause”指令，这条指令从奔腾4处理器开始引入，目的就是为了向处理器暗示正在实行自旋-等待的循环当中。这带来的好处有两点，一是处理器能够因此避免错误顺序的访存从而避免性能惩罚（这种错误顺序的访存与流水线以及分支预测有关）；二是处理器不用反复快速的执行xchg指令从而节省功耗。这条指令广泛存在于自旋锁的实现中。

其实说到这里我们可能会觉得原子性是理所当然的可以保证了，实际上自旋锁的实现也确实完成了。但是之所以这样子的实现满足原子性是因为在硬件层面上，对于原子性有足够多的支持。对于自旋锁的原子性而言，最为重要的一点其实就在于xchg指令的原子性：只要我们能够保证xchg指令在取出锁变量值，与相应的变量值交换并存入锁变量这一整个过程中不会有被中断的可能性，那么我们就可以认为自旋锁不论是上锁还是解锁都是具备原子性的。可问题是这一系列的操作显然不止需要一个时钟周期，硬件架构显然不可能通过让多个处理器在这个操作的时候都停下来从而保证操作不被中断。实际上，x86架构使用总线锁来保证类似于xchg指令的原子性。总线锁的体现形式是LOCK#信号，当这个信号生效时，其他处理器对于总线的使用请求都会被阻塞住，从而保证了其他处理器没有办法取用当前指令需要读写的内存位置。对于x86体系中的原子指令，比如xchg，这个信号是自动开启的；对于其他的指令，软件可以通过在指令前面加上前缀“lock”来显示的开启这个信号，从而阻塞总线。除了xchg指令之外，多核架构下更新段描述符，更新页表等状况下也会自动启用这个信号。

除了LOCK#信号之外，从Intel486处理器开始，基本的读写内存中对齐位置的数据是保证原子性的。由此可以想到，在实现解锁操作的时候，我们可以直接用mov指令将锁变量置为0，这显然对于自旋锁的性能是一种提升。不过在部分处理器架构上，这种原子性并没有办法保证；由于处理器架构有很多而且细节繁多，这里不再讨论。不过xchg()指令一个明显的问题在于，不论解锁是否是成功，都会反复的写内存；实际上大多数的写操作都不必要的，取代的方式可以是只有当发现了内存值变化了之后才去尝试写锁变量。

对于自旋锁来说，因为它的特点是轻便，只涉及到用户层的参与，所以不需要内核为之提供系统调用。在操作系统中（尤其是对内核代码加了内核锁的JOS），内核也是一种珍贵资源，自旋锁不断的旋转（循环），直到等待的资源被释放或者分配给线程的时间片用光，节省了内核资源但是
耗费了大量的计算资源，一般只对小型的计算资源施加，同时也被用于实现其他的锁。


##3 互斥锁
和自旋锁一直占用处理器资源的“忙等待”不同，互斥锁代表了另外一种等待资源的方式。互斥锁在发现资源被占用的时候，会主动交出对计算资源的占用，这就涉及到了对于内核的请求。和自旋锁一样的是，互斥锁也有一个锁变量，这个锁变量的初始值为1，代表资源可用；当锁变量小于1的时候，代表资源被占用。理论上不应该出现锁变量大于1的情况，在POSIX中指出，当对一个未上锁的互斥锁解锁的时候结果是未定义的，我也就不对此作过多讨论。类似的情况还有，当一个线程试图解锁其他线程锁上的互斥锁，同样这种操作的结果是未定义的，操作系统不对这样的行为有明确的禁止，但是不保证其结果会是怎样的；实际上在基本的互斥锁还有几种变种，它们对于加锁和开锁的条件有不同的限制。互斥锁的加锁操作首先检查锁变量的值，如果是1，那么资源可用，锁变量值减1；否则利用系统调用将当前线程挂起，等待资源释放的时候操作系统唤醒。互斥锁的解锁操作直接从等待队列中选取一个线程唤醒，该线程从而能够访问临界区的资源；有关唤醒的策略是多种多样的，不过不是这里关心的重点，唤醒队列中靠前的线程。将互斥锁有关的操作应用到操作系统中会对流程有一些相应的改变。

要考虑的最重要的问题就是进行一次加锁/解锁的开销是怎样的，是不是在操作系统能够接受的范畴内。不同于自旋锁的“轻便”，互斥锁每次开锁和上锁都需要进行系统调用和上下文切换，由于操作系统对于内核有不同粒度的锁，所以对内核的过度占用会显著的影响整个操作系统的性能。试想，在很多情况下，线程之间并没有对资源有激烈的争抢，多个处理器较为有序的轮流进入临界区，此时每次调用pthread_mutex_lock()实际上不需要经由内核挂起线程；如果在这种情况下依然要求线程使用系统调用，那么将会是很大的资源浪费；同样，在解锁操作中可能并没有其他线程在等待资源，此时并不需要通过系统调用唤醒线程。这是对互斥锁操作进行演进的一个方向：将对锁变量的操作放在用户空间中，在没有资源竞争的情况下不调用系统调用，只需要修改锁变量的值，当资源不可得的时候请求内核将线程挂起；至于解锁操作，只有当资源可用而且有线程在等待的时候才使用系统调用来唤醒。

这也就是futex(fast userspace mutex)被引入的原因。其本质是存在于内核中的等待队列，其中是因为等待资源（互斥锁）释放的线程。内核根据用户空间中锁变量对应的用户空间地址来索引在这个锁变量下阻塞住的线程等待队列，一般操作系统使用哈希表来进行映射。对于futex的操作被封装成了系统调用sys_futex()（这是unix系统中的经典实现，因此在这里我们也进行沿用）。该系统调用的第一个参数指明了用户空间中对应锁变量的用户空间地址，第二个参数指明了对于等待队列的具体操作是什么，其值可为FUTEX_WAIT（即请求挂起当前线程）或FUTEX_WAKE（即释放资源告诉内核唤醒等待资源的线程）。

为了实现futex相关的操作，我在每个进程的控制块中增加了一个数组，其中的每一个元素都是一个互斥锁变量到与之相关等待队列的映射，而等待队列是一个元素为进程控制块指针的数组。用户程序在挂起线程的时候，根据锁变量地址来索引相应的等待队列；为了降低实现的复杂度，我并没有使用哈希函数来索引，而是使用顺序搜索来进行查找锁变量的地址；之后的操作在等待队列中针对元素进行操作。

互斥锁的内核部分是futex，而其用户空间部分就是前文中提到的锁变量。不过在增加了分离式的结构之后，我们对其值的操作和理解有了一些改变。当锁变量的值为1的时候我们还是认为资源可用，而对于上锁操作我们直接对其值减一，因此成功的上锁操作在减一之后锁变量的值就成为了0，此时认为资源成功被当前线程占用；而失败的上锁操作在减一之后锁变量成为负值，此时调用sys_futex(&lock, FUTEX_WAIT)，将当前线程挂起。在解锁操作中，直接在锁变量上加一，之后我们需要考虑是否唤醒等待线程：如果加一之后锁变量值为1（也就是初始值），那说明此时在互斥锁上的减一和加一数目相同，没有线程在等待资源，不用系统调用；否则说明有线程在等待，调用sys_futex(&lock, FUTEX_WAKEUP)，通知内核唤醒相关的等待线程。

到目前为止所有的概念和流程都是直接而符合直觉的，但是为了达成底层的原子性，还需要添加一系列的机制来支持。首先是对锁变量的操作应该是原子性的，不然在多个线程同时对锁变量增减时就会出现不一致的情况。有多种实现方式，在有些操作系统中，一个互斥锁是一个结构体，其中成员除了锁变量之外还有一个自旋锁，所有对锁变量值的改变都需要在自旋锁的辅助下完成；而在我的实现中，对锁变量值的改变是通过原子操作xadd实现的，也就是fetch_and_add，这个指令保证变量值从读取到改变整个过程中不会被破坏。然而即使是这样子，原子性依然没有得到保证。试想当线程A调用pthread_mutex_lock中对锁变量进行了fetch_and_add(&mutex, -1)，然后发现锁变量此时值为-1，表示应该将当前线程挂起等待资源释放之后被唤醒；但是这之后锁变量就不受保护了，正当准备进入系统调用挂起的时候，另外一个占用资源的线程B调用了pthread_mutex_unlock，对锁变量进行了fetch_and_add(&mutex, 1)释放资源并先于线程A进入系统调用打算唤醒线程（实际上此时也没有线程在挂起，因为A还没有进入系统调用）。在这种情况下线程A如果挂起自己，那么永远不会被唤醒了，所以我们需要在系统调用内部多检查一次锁变量的值。对于sys_futex(FUTEX_WAKE)操作，如果系统调用中发现锁变量值为0，那么意味着在本次上锁之前锁变量值为1（也就是资源空闲），系统调用直接返回而不用将线程挂起。相应的，解锁操作中也会因为类似的原因出现没有线程可以唤醒的情况，直接返回即可。

对于互斥锁而言，反复频繁的线程切换是影响性能的主要因素，所以它并不适合资源竞争很“剧烈”的情况，一般可以通过缩小临界区代码量的方式来增加效率。一种对互斥锁的改进方式是和自旋锁结合：先使用自旋锁的循环经过一定的次数之后，如果资源一直没有释放，再使用互斥锁来切换上下文。

##4 信号量
互斥锁有时候被称作“二元信号量”，但从实现上来说二者的区别是很显著的。互斥锁被认为是用来进行线程之间的互斥，也就是说当一个线程占用资源的时候，排斥所有其他的线程对资源进行访问；而信号量被用来同步多个线程/进程之间的执行，这种同步有时候会允许多个控制流对于资源的同时访问，不过会确保它们执行是有序的。实际上，信号量是IPC的一种，管理进程之间对共享资源的取用，但进程显然不是本文关注的重点，所以使用仅限于线程之间。另外，一个线程/进程对信号量的“上锁”操作，很多时候可能是由另外一个线程/进程“解锁”的；而互斥锁的概念中并不认为这样子的操作是有效的。对于这两个概念间的关系有很多种说法，甚至有人会把mutex当作semaphore的特例。在这里我主要的目的是实现counting semaphore，也就是计数信号量，在此基础上的应用仅限于PV操作。

在POSIX标准下的信号量并不是一个“锁变量”，而是一组信号量值，在申请的时候可以在semget()库函数的nsem参数中声明需要取用多少个信号量。在用户程序调用这个库函数的时候可以指明一个key值，这个值在内核内部映射到一组信号量的数据结构上；之所以有这个设计是因为信号量本身是多个进程之间共享的，用户程序需要通过这种方法告诉内核自己想要的是哪组信号量。实际上，信号量也正是为了方便多进程共同取用才会在大多情况下在内核中实现，由于进程间的数据交互不用像线程那样对性能有很高的要求，所以这里不用考虑开销的问题。semget的最后一个参数是oflag，表示与创建和权限有关的标志位，在我的实现中由于时间有限，出于控制复杂度的考虑只支持简单的IPC_CREAT和IPC_EXCL参数。这里对相关的数据结构进行一些说明：
		struct semid_ds {
			unsigned short sem_nsem;
			uint32_t key; 
			envid_t sempid;
			struct {
				unsigned short semval;
			} semset[MAXSEMNM];
		}
这个数据结构相当于一个信号量集合，sem_nsem表示集合中有效信号量的数目，这个是根据信号量创建时候定义的；key是创建的时候集合绑定的值，所有进程/线程都通过这个值来索引信号量；sempid是创建信号量的进程id；最后的一个匿名结构数组中是信号量值的数组，在POSIX标准下还有一些其他的成员，这里只保留了信号量的值。在内核中有个信号量数据结构的数组，表示了系统中可用的信号量资源；
		struct semid_ds semaphore[SEMSETNM] = {0};
在用户程序调用semget()时，先从这个数组里面搜索对应key值的信号量。如果找到了，而用户添加了IPC_CREAT和IPC_EXCL参数，说明已有绑定的信号量返回错误值，否则返回信号量数组的数组下标作为semid。如果没有找到而没有添加IPC_CREAT,则返回错误值；否则在信号量数组中找到一个空闲的信号量并且按照参数初始化相应的成员变量。

在用户程序获取了信号量之后，会接着调用semctl()函数来赋初始值。这个函数可以用来完成很多功能，都是通过参数cmd来指示的，不过在这里只用到SETVAL（赋值）和IPC_RMID（删除信号量）两个。semctl()通过系统调用来完成相应的功能，都是对semid对应的信号量结构进行操作，实现起来比较简单。

### semop()
信号量最为重要的操作是semop()，这也是实现PV操作的基础，和互斥锁一样都需要借助futex来完成等待队列。和互斥锁的上锁/解锁操作不同，对于信号量的操作都是通过sembuf结构体来指示的。
		struct sembuf {
			unsigned short sem_num;
			short          sem_op; 
			short          sem_flg;
		};
三个成员变量分别指示了操作目标在信号量数组的下标，操作对于目标信号量的增量以及标志位（因为我们的目的是实现PV操作，这个标志位不作实现和讨论）。这样设计的原因在于，semop接受的参数是sembuf结构的数组，方便用户程序在进行同步的时候一次加入一个序列的操作。对于每一个semop的操作，分为两种情况：sem_op大于0或者小于0（等于0的情况这里不作讨论）。

当sem_op小于0的时候表示希望占用资源，只有当sem_num指示的信号量在加sem_op之后大于等于0 的情况下才可以加这个值，表示占用资源；否则说明资源不够用，需要将当前线程挂起等待资源的释放。这里要说明的一点是，由于我们在实现futex的时候，由于要保证互斥锁操作的原子性，当进入系统调用发现“锁变量”的值为0则直接返回；而不巧的是在信号量这里需要挂起线程的时候很可能信号量的值刚好为0，所以在调用sys_futex(&sem, FUTEX_WAIT)之前先减一个值，调用之后再加。这里不用担心原子性的问题是因为所有对信号量的操作都在内核中进行，内核锁的存在保证了操作是安全的。之所以在互斥锁和信号量中对于变量的值采取不同的处理方式（互斥锁是先改变值再检查，信号量是先检查再改变），是因为在互斥锁中需要用原子操作保证整个流程的原子性，如果先检查值再改变会更复杂。

sem_op大于0表示释放资源。由于信号量最小为0，因此直接调用futex唤醒即可，这里不用担心系统调用的成本了，因为本身就在系统调用中；之后对信号量增加sem_op。

和互斥锁的实现不一样的是，当因为信号量而被挂起的线程在被唤醒的时候不可以直接取用资源，因为之前被挂起之前并没有对信号量的值进行过改变，所以需要再次尝试操作。因此，这里需要有一个循环操作，重复对目标信号量进行改变直到成功，从而可以占用资源。这里要注意的一点是，如果把这个循环放在sys_semop系统调用中的话，将导致死锁；原因在于这个循环的终止条件是另外一个线程通过系统调用增加信号量的值，而内核一直被循环占用因此无法响应其他线程的请求。所以semop也是分离结构。注意到semop实际上是要进行一整个sembuf数组的操作，所以也需要循环来遍历数组。整理一下就是，sys_semop返回值为0的时候表明成功，小于零的时候表示错误，而大于零的时候表示的是需要重复的操作在操作数组中的下标（从1开始的）；每次semop调用sys_semop都从需要重复的操作开始。

但是在实现完信号量的有关api以及测试程序之后，出现了一个问题：当测试程序执行完之后有时候会出现页错误，具体的位置是在线程执行终止例程的过程中。在线程的终止进程中，唯一的行为就是用sys_thread_destroy()系统调用回收线程有关的资源，而这个系统调用保证控制流不会回到要销毁的线程上去。而页错误刚好就出现在系统调用之后在用户线程上执行的第一条pop指令，这条指令错误是因为此时线程栈已经被释放了，所以现在的问题就是为什么控制流又回到了被释放的线程上。sys_thread_destroy()为了让线程不再继续执行做了两件事：标记用户线程为FREE，调用sched_yield()。标记线程状态为FREE是直接的，所以错误更应该出在sched_yield()中，但是检查了这个函数之后发现应该没有错误，而通过输出调试，发现这个线程在销毁例程之后确实在另一个线程上通过scheduler被再次调用了而且其状态竟然变成了RUNNABLE。线程/进程控制块是内核资源，所以即使是不同线程之间也不可能出现不一致性。经过长时间的排查我发现，错误原因原来是当线程1终止并被标记为free了之后，线程2执行了一次释放信号量的操作，并且进一步在sys_futex(&sem, FUTEX_WAIT)中将线程1标记为RUNNABLE，这是因为尽管线程1的控制块被清空了，但是在信号量下的等待队列中依然还保存了指向这个控制块的指针。这里需要在sys_futex中加入对准备唤醒的线程当前状态的判断，只有为NOT_RUNNABLE的时候才置为RUNNABLE。从这个错误中我意识到，操作系统是一个错综复杂的系统，造成一个错误现象的来源可能是看起来和错误关系不大的某一部分；同时，在内核代码中，及时的释放掉不需要的虚拟页是监测错误的重要手段。

##5 封装与测试
第一阶段的测试是和线程的基本api有关的，包括线程的创建，线程的终止以及线程的终止等待。测试程序在开始的时候输出main函数的开始信息，然后一次在mythread例程上面创建四个线程，例程的内容是输出参数字符串的内容。然后在主线程中调用pthread_join来等待四个线程的终止，最后输出结束信息。期望的输出结果应该是，首先是main函数的开始信息，然后是四个线程分别输出其参数字符串（还包括线程的控制块编号，运行的CPU编号），理论上来说四个线程的输出顺序应该是随机的，不过由于每个线程执行的工作很少，可能结果是按创建顺序输出的，但是承担线程的CPU分配每次都是不同的。

我尝试了	在每个线程中循环执行一些冗余的计算，可以得到乱序的输出。

在不调用pthread_join等待子线程结束的时候可以预期的结果是子线程不一定会输出而是在主线程退出的时候被强制退出。实现错误的情况是线程继续执行，从而会触发页错误，这将会输出页错误信息。



对于线程的自旋锁和互斥锁，测试的方法是使用计数例程，多个线程同时向一个变量作加法；作为对照有程序不加锁，得到错误的答案。


对于信号量的操作是基于PV操作的，有一个计数线程和一个输出线程，计数线程每次计数了之后输出线程才能输出，输出线程输出了一个值之后计数线程才能够进行下一轮的计数。



##6 不足与展望
在对JOS进行线程有关的功能拓充的同时，我很清醒的意识到，在有限的时间内靠一己之力完成一个功能完备的操作系统是不现实的。在实现必须功能的同时，了解一个概念的基本实现就已经足够了，在此基础上能够尝试去了解这个实现的缺点何在，有哪些改进措施，如何优化以及成熟的操作系统是如何实现的。这里大致的总结一下在实现功能的过程中有哪些缺失。

最需要说明的问题实际上是在多核架构上，这作为多进程多线程的立足点，有些问题是没有深入讨论说明的。试想，多个处理器在并行执行不同进程里的线程时，cr3寄存器的值应该是怎样的？实际上，每个CPU的核心都有自己的cr3寄存器，那么其中的值应该是根据当前执行进程的地址空间决定的。实际上在本文中没有谈到的包括各级cache机制，比如TLB以及处理器中的缓存，这些机制在多线程的环境下影响程序的一致性（比如，如何保证当缓存中的值失效的时候及时发现。在cache存在的情况下，指令的原子性还是由硬件保证的，不过这个话题在这里不再多做讨论。至于TLB，操作系统在很多时候需要刷新tlb中的内容，比如在修改页表中的条目时候，在这里不再研究更多的细节。实际上，关于多核架构下的多线程有关的硬件支持要比文中讨论的复杂很多，这里只能做基本的研究。

在数据结构方面，为了把精力集中在系统的设计上，我尽量使用的是最为简单的数据结构。比如在实现等待队列的时候我并没有使用链表而是使用定长数组，在实现futex
中锁变量地址到等待队列映射的时候，实际上应该是使用哈希函数，包括实现信号量的时候也没有去讨论和研究最为合适的数据结构。实际上，数据结构的简化为前期的调试带来了一定的方便，如果有需要也可以对数据结构进行相应的优化。

在线程的基本api上，也有很多简化，大体上来说我只实现了线程基本的api及其基本的功能。比如pthread_create没有支持各种各样的创建参数，比如不支持用户指定线程的堆栈等；在线程的pthread_join函数中，我并没有实现线程的返回值，因为一般来说线程的返回值是需要用malloc在动态分配的，而JOS中并没有实现内存的动态分配；自旋锁和互斥锁的实现也没有包括锁的一些自定义特性；semctl操作只实现了赋值和删除两个功能；没有实现pthread_cancel等线程api。

另外，我没有实现同一进程内的多个线程对异常栈的互斥使用。在JOS中，内核为用户程序提供了异常处理的接口，当程序出现异常的时候，可以对某一些类型的异常添加用户自定义的异常处理例程。一个最为典型的例子是缺页错误，一种可能的页错误是JOS的fork函数采取的是COW(copy on write)策略，即当新进程创建的时候，地址空间中用户空间页面都没有从旧进程地址空间中拷贝过来，而是共用之前的物理页面只拷贝一份映射；当任意一个进程对共用页面进行写操作的时候，才会真正的创建出新的一份页面拷贝。这样做的原因在于，很多时候fork函数被调用之后紧接着就会调用exec()函数，这个函数会将新进程的内存替换为新的程序，使用COW就会避免很多不必要的开销。在类似的异常处理情况下，使用的堆栈是特殊的异常栈，位于进程地址空间中用户部分的顶部，每个进程只有一份，如果要多个线程共用的话就意味需要进行很多额外的讨论。为了避免问题扩大化，这里忽略了线程中用户自定义异常处理的问题。

在实现同步原语的时候，除了正确性之外很重要的因素是考虑在实际应用中的性能，不过由于实验设备以及课题难度的因素，严格的对性能进行比较和优化需要过多的时间，实际上这也不是课题的重心。



